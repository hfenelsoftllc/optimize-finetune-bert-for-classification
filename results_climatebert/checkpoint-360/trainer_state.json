{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.72972972972973,
  "eval_steps": 36,
  "global_step": 360,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 2.2946226596832275,
      "learning_rate": 3.6e-06,
      "loss": 1.0998,
      "step": 10
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 1.9246467351913452,
      "learning_rate": 7.6e-06,
      "loss": 1.0974,
      "step": 20
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 2.381572961807251,
      "learning_rate": 1.16e-05,
      "loss": 1.0888,
      "step": 30
    },
    {
      "epoch": 1.0810810810810811,
      "grad_norm": 2.329271078109741,
      "learning_rate": 1.56e-05,
      "loss": 1.089,
      "step": 40
    },
    {
      "epoch": 1.3513513513513513,
      "grad_norm": 1.5945730209350586,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 1.0825,
      "step": 50
    },
    {
      "epoch": 1.6216216216216215,
      "grad_norm": 1.0420360565185547,
      "learning_rate": 2.36e-05,
      "loss": 1.0784,
      "step": 60
    },
    {
      "epoch": 1.8918918918918919,
      "grad_norm": 2.5895752906799316,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 1.0839,
      "step": 70
    },
    {
      "epoch": 2.1621621621621623,
      "grad_norm": 2.6356308460235596,
      "learning_rate": 3.16e-05,
      "loss": 1.0452,
      "step": 80
    },
    {
      "epoch": 2.4324324324324325,
      "grad_norm": 2.073204278945923,
      "learning_rate": 3.56e-05,
      "loss": 1.0628,
      "step": 90
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 2.154478073120117,
      "learning_rate": 3.960000000000001e-05,
      "loss": 1.0502,
      "step": 100
    },
    {
      "epoch": 2.972972972972973,
      "grad_norm": 0.9634051322937012,
      "learning_rate": 4.36e-05,
      "loss": 1.0541,
      "step": 110
    },
    {
      "epoch": 3.2432432432432434,
      "grad_norm": 1.2379980087280273,
      "learning_rate": 4.76e-05,
      "loss": 1.0231,
      "step": 120
    },
    {
      "epoch": 3.5135135135135136,
      "grad_norm": 2.8920464515686035,
      "learning_rate": 5.16e-05,
      "loss": 1.0216,
      "step": 130
    },
    {
      "epoch": 3.7837837837837838,
      "grad_norm": 2.4797542095184326,
      "learning_rate": 5.560000000000001e-05,
      "loss": 1.0467,
      "step": 140
    },
    {
      "epoch": 4.054054054054054,
      "grad_norm": 2.5873513221740723,
      "learning_rate": 5.96e-05,
      "loss": 1.007,
      "step": 150
    },
    {
      "epoch": 4.324324324324325,
      "grad_norm": 1.92379891872406,
      "learning_rate": 6.36e-05,
      "loss": 0.9693,
      "step": 160
    },
    {
      "epoch": 4.594594594594595,
      "grad_norm": 3.2215938568115234,
      "learning_rate": 6.76e-05,
      "loss": 0.9445,
      "step": 170
    },
    {
      "epoch": 4.864864864864865,
      "grad_norm": 2.4311978816986084,
      "learning_rate": 7.16e-05,
      "loss": 0.9205,
      "step": 180
    },
    {
      "epoch": 5.135135135135135,
      "grad_norm": 1.7294385433197021,
      "learning_rate": 7.560000000000001e-05,
      "loss": 0.833,
      "step": 190
    },
    {
      "epoch": 5.405405405405405,
      "grad_norm": 2.387399911880493,
      "learning_rate": 7.960000000000001e-05,
      "loss": 0.7912,
      "step": 200
    },
    {
      "epoch": 5.675675675675675,
      "grad_norm": 2.7575652599334717,
      "learning_rate": 8.36e-05,
      "loss": 0.7607,
      "step": 210
    },
    {
      "epoch": 5.945945945945946,
      "grad_norm": 2.418022871017456,
      "learning_rate": 8.76e-05,
      "loss": 0.7338,
      "step": 220
    },
    {
      "epoch": 6.216216216216216,
      "grad_norm": 2.855306386947632,
      "learning_rate": 9.16e-05,
      "loss": 0.7026,
      "step": 230
    },
    {
      "epoch": 6.486486486486487,
      "grad_norm": 3.837207078933716,
      "learning_rate": 9.56e-05,
      "loss": 0.5844,
      "step": 240
    },
    {
      "epoch": 6.756756756756757,
      "grad_norm": 3.9555740356445312,
      "learning_rate": 9.960000000000001e-05,
      "loss": 0.7125,
      "step": 250
    },
    {
      "epoch": 7.027027027027027,
      "grad_norm": 2.474367141723633,
      "learning_rate": 0.00010360000000000001,
      "loss": 0.6217,
      "step": 260
    },
    {
      "epoch": 7.297297297297297,
      "grad_norm": 2.4058282375335693,
      "learning_rate": 0.00010760000000000001,
      "loss": 0.5747,
      "step": 270
    },
    {
      "epoch": 7.5675675675675675,
      "grad_norm": 2.0935771465301514,
      "learning_rate": 0.00011160000000000002,
      "loss": 0.5486,
      "step": 280
    },
    {
      "epoch": 7.837837837837838,
      "grad_norm": 3.4409306049346924,
      "learning_rate": 0.00011559999999999999,
      "loss": 0.5526,
      "step": 290
    },
    {
      "epoch": 8.108108108108109,
      "grad_norm": 3.0828161239624023,
      "learning_rate": 0.00011960000000000001,
      "loss": 0.6197,
      "step": 300
    },
    {
      "epoch": 8.378378378378379,
      "grad_norm": 2.1806607246398926,
      "learning_rate": 0.0001236,
      "loss": 0.5072,
      "step": 310
    },
    {
      "epoch": 8.64864864864865,
      "grad_norm": 3.157541036605835,
      "learning_rate": 0.0001276,
      "loss": 0.4796,
      "step": 320
    },
    {
      "epoch": 8.91891891891892,
      "grad_norm": 4.408993244171143,
      "learning_rate": 0.0001316,
      "loss": 0.4595,
      "step": 330
    },
    {
      "epoch": 9.18918918918919,
      "grad_norm": 3.7349867820739746,
      "learning_rate": 0.00013560000000000002,
      "loss": 0.6933,
      "step": 340
    },
    {
      "epoch": 9.45945945945946,
      "grad_norm": 4.7518157958984375,
      "learning_rate": 0.0001396,
      "loss": 0.4387,
      "step": 350
    },
    {
      "epoch": 9.72972972972973,
      "grad_norm": 3.038372755050659,
      "learning_rate": 0.0001436,
      "loss": 0.4525,
      "step": 360
    }
  ],
  "logging_steps": 10,
  "max_steps": 370,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 36,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 757298730240000.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
